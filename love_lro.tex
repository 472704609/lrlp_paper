\documentclass[11pt]{article}

% \usepackage{default}
% \usepackage{fullpage}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}


\bibliographystyle{plain}

\author{David~Love and G\"{u}zin~Bayraksan}

\title{A Likelihood Robust Method for Water Allocation under Uncertainty}
\date{}

% Frequently used general mathematics
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Rp}{\R^+}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Zp}{\Z^+}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}

% Commands for probability
\renewcommand{\P}{\mathbb{P}}
\newcommand{\p}[1]{\P \left[ #1 \right]}
\newcommand{\e}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\ee}[2]{\mathbb{E}_{#1} \left[ #2 \right]}

% Definitions of variables
\newcommand{\X}{X}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xh}{\hat{\x}}
\newcommand{\lh}{\hat{\lambda}}
\newcommand{\mh}{\hat{\mu}}
\newcommand{\xs}{\x^*}
\newcommand{\xit}{\tilde{\mathbf{\xi}}}
\newcommand{\zt}{\tilde{z}}
\newcommand{\zs}{z^*}

% Further variables
\newcommand{\y}{\mathbf{y}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\q}{\mathbf{q}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\d}{\mathbf{d}}

% Useful mathematics functions
\newcommand{\keywords}[1]{\par\noindent\enspace\ignorespaces\textbf{Keywords:} #1}
% \newcommand{\keywords}[1]{\par\addvspace\baselineskip\noindent\keywordname\enspace\ignorespaces #1}
\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\st}{\mbox{s.t.}}

\begin{document}

\maketitle

\begin{abstract}
	We adapt the Likelihood Robust Optimization method recently proposed by Wang, Glynn and Ye to examine water allocation under the ambiguous distribution of future available supply and demand. We examine the value of collecting additional data and the cost of finding a solution robust to an ambiguous probability distribution. A decomposition-based solution algorithm to solve the resulting model is given. Computational results are presented on a long-term water allocation problem in southeast area of Tucson.
\end{abstract}

\section{Introduction \& Motivation}


More than 25 million people in the southwestern United States depend on the water supplied by the Lower Colorado River Basin for their livelihood. More than half of Tucson's water, for instance, comes from this source. The Colorado River Basin has experienced a sustained period of drought in recent years, which has led to questions about the adequacy of the Colorado to meet future demands, especially as the population of Arizona (and of other states that depend on this water source) increases. Thus, the problem of allocating Colorado water is of critical importance. 
 
% The reliability of the Colorado River system under future climate variability is critically important to the long-term well-being of Arizona and the other six states that depend on this water supply \cite{usbr_colorado_climate}. The current approach to modeling the water supply is to use Global Circulation Models (GCM) to generate regional rainfall and temperature scenarios by the so-called statistical downscaling techniques \cite{christensen_lettenmaier_07,dibike_caulibaly_05}.  The US Bureau of Reclamation also uses an approach called the {\it scenario planning} that examines water demand and supplies over the next 50 years \cite{usbr_11}.

In this paper, we present a generalized network model of Colorado River water allocation in Tucson, Arizona motivated by the CALVIN (CALifornia Value Integrated Network) optimal water allocation model of California created by Draper et.\ al.\ \cite{draper_etal_03}.  More general models of Colorado River water distribution have also been studied, such as the Colorado River Reservoir Model \cite{christensen2004effects} and the Colorado River Budget Model \cite{barnett2009sustainable}.  Our model is modified to incorporate future uncertainty by using the Likelihood Robust Optimization (LRO) approach of Wang, Glynn and Ye \cite{wang2010likelihood}.  LRO is a data-driven method that uses observations of random or unknown parameters to account not only for inherent stochasticity of a problem, but the ambiguity of the probabilistic model itself.  This is especially important in our application as we are looking 40 years into the future and there is considerable ambiguity in the uncertainties.

The LRO is especially attractive because only those scenarios of interest, obtained either through observation or simulation, are used directly in the calculations.  Thus the size of the problem is polynomial in the sample size, making it computationally tractable.  Furthermore, the samples used in the LRO can represent select scenarios that are of interest to the authorities---for instance, generated by the scenario planning process \cite{cityofTucsonWaterPlan,usbr_11}. This fits with practice well; the scenario planning process results in several scenarios that the agencies would especially like to be robust against.

This paper is organized as follows: in Section \ref{sec:network_model} we state the generalized network model in deterministic and stochastic forms; in Section \ref{sec:lrlp2} we describe the Likelihood Robust Optimization method for a two-stage stochastic linear program with recourse; in Section \ref{sec:value} we discuss a method of measuring the value of gathering additional data for input to the LRO; in Section \ref{sec:soln_algorithm} we present a decomposition method for solving the LRO mode; and in Section \ref{sec:comp_results} we present some computational results for our model.

\section{Generalized Network Water Model} \label{sec:network_model}

We begin with a generalized network flow model of Colorado River water allocation in Tucson.  The deterministic model is a multi-year model of the form
\begin{align*}
	\min_{x,s,r} \ & \sum_{i,j} \sum_t c_{ijt}^x x_{ijt} + \sum_{j} \sum_{t} c_{jt}^s s_{jt}\\
	\st \ & \sum_i x_{jit} + s_{j,t+1} + r_{jt} = \sum_i a_{ijt} x_{ijt} + s_{jt} + b_{jt}, \ \forall j,t \\
	& l_{ijt}^x \leq x_{ijt} \leq u_{ijt}^x, \ \forall i,j,t \\
	& l_{jt}^s \leq s_{jt} \leq u_{sjt}^s, \ \forall j,t \\
	& l_{jt}^r \leq r_{jt} \leq u_{sjt}^r, \ \forall j,t,
\end{align*}
where $x_{ijt}$ is the flow from node $i$ to node $j$ during the $t^{\text{th}}$ time period, $s_{jt}$ is the storage in node $j$ at the beginning of the $t^{\text{th}}$ time period, $r_{jt}$ is the water released to surface water from node $j$ in time period $t$, $b_{jt}$ is the external inflow into node $j$ in time period $t$, $a_{ijt}$ is the gain/loss of flow on arc $(i,j)$ at time $t$, $c_{ijt}^x$ and $c_{jt}^s$ are the economic losses associated with arc $(i,j)$ at time $t$ (representing energy and treatment costs) and storage in node $j$ respectively, and $l_{ijt}^x$, $l_{jt}^s$, $l_{jt}^r$, $u_{ijt}^x$, $u_{jt}^s$ and $u_{jt}^r$ are the lower and upper bounds on decision variables.

To make this model stochastic we rewrite it as a two-stage linear program.  The model has a total of $P = 41$ time periods, representing years 2010-2050, which will be split into two stages of $P_1$ and $P_2 = P - P_1$ years each.  Second stage parameters $b_{jt}$ and (some) lower and upper bounds are then assumed to be stochastic.  The model thus becomes
\begin{align}
	\min_{(x,s,r) \in L^1} \ & \sum_{i,j} \sum_{t=1}^{P_1} c_{ijt}^x x_{ijt} + \sum_{j} \sum_{t=1}^{P_1} c_{jt}^s s_{jt} + \sum_{\omega=1}^n p_\omega h^\dagger_\omega(s) \label{eq:gen_network_two_stage} \\
	\st \ & \sum_i x_{jit} + s_{j,t+1} + r_{jt} = \sum_i a_{ijt} x_{ijt} + s_{jt} + b_{jt}, \ \forall j, 1 \leq t \leq P_1 \notag
\end{align}
where
\begin{align}
	h^\dagger_\omega(s) = \min_{(x,s,r) \in L^2_\omega} \ & \sum_{i,j} \sum_{t=P_1+1}^{P} c_{ijt}^x x_{ijt} + \sum_{j} \sum_{t=P_1+1}^{P} c_{jt}^s s_{jt} \label{eq:gen_network_second_stage} \\
	\st \ & \sum_i x_{jit} + s_{j,t+1} + r_{jt} = \sum_i a_{ijt} x_{ijt} + s_{jt} + b_{jt}^\omega, \ \forall j, P_1+1 \leq t \leq P. \notag
\end{align}
The $\omega$ index indicates the $n$ second-stage scenarios with probabilities $p_\omega$ and $L^1$ and $L^2_\omega$ represent the feasible regions defined by the lower and upper variable bounds.

In the rest of the paper, we simplify the notation for the first-stage (\ref{eq:gen_network_two_stage}) and second-stage (\ref{eq:gen_network_second_stage}) problems as follows: in the first-stage, decision variables $\{x_{ijt}\}$, $\{s_{jt}\}$ and $\{r_{jt}\}$ become the vector $\x$, costs $\{c_{ijt}^x\}$ and $\{c_{jt}^s\}$ are written as the row vector $\c$, the supply parameters $b_{jt}$ become the vector $\b$ and the constraint matrix is written as $A$.  In the second stage we denote the decisions as $\y^\omega$, the costs as $\q^\omega$, the demands as $\d^\omega$, and the constraint matrices multiplying $\y^\omega$ and $\x$ as $D^\omega$ and $B^\omega$, respectively.  Note that the problem now has the form of a general two-stage stochastic linear program with recourse.

\section{LRLP-2 Formulation} \label{sec:lrlp2}

To apply the likelihood robust optimization framework we will for now begin with a general two-stage stochastic linear program with recourse (SLP-2), and use it to form a two-stage likelihood robust linear program with recourse, which will will denote as LRLP-2.  The SLP-2 is formulated as
\begin{align*}
	\min_\x \ & \c\x + \e{h^\dagger(\x)} \\
	\st \ & A\x = \b \\
	& \x \geq 0
\end{align*}
where
\begin{align}
	h^\dagger_\omega(\x) = \min_{y^\omega} \ & \q^\omega y^\omega \label{eq:slp_second_stage} \\
	\st \ & D^\omega y^\omega = B^\omega \x + \d^\omega \nonumber \\
	& y^\omega \geq 0. \nonumber
\end{align}
We assume a finite number of scenarios indexed by $1 \leq \omega \leq n$ with corresponding probabilities $p_1, \dots, p_n$.  For simplicity we assume relatively complete recourse, i.e., the second-stage problems $h^\dagger_\omega(\x)$ are feasible for every feasible solution $\x$ of the first-stage problem.  To derive the LRLP-2, we begin by writing SLP-2 in extensive form
\begin{align*}
	\min_{\x,y^\omega} \ & \c\x + \sum_\omega p_\omega \q^\omega y^\omega \\
	\st \ & A\x = \b \\
	& -B^\omega \x + D^\omega y^\omega = \d^\omega\ \forall \omega \\
	& \x \geq 0 \\
	& y^\omega \geq 0\ \forall \omega.
\end{align*}

In the likelihood robust formulation the distribution $p_\omega$ is unknown, but scenario $\omega$ has been observed $N_\omega$ times, with $N = \sum_{\omega=1}^n N_\omega$ total observations.  The objective function is minimized relative to the worst-case distribution selected from a set of distributions with sufficiently high likelihood $\prod_{\omega=1}^n p_\omega^{N_\omega}$.  Its deterministic equivalent form is
\begin{align}
	\min_{\x,y^\omega} \max_p \ & \c\x + \sum_\omega p_\omega \q^\omega y^\omega \label{eq:lrlp_primal}\\
	\st \ & A\x = \b \nonumber \\
	& -B^\omega \x + D^\omega y^\omega = \d^\omega\ \forall \omega \nonumber \\
	& \sum_\omega N_\omega \log p_\omega \geq \gamma & (\lambda) \label{eq:lrlp_primal_liklihood} \\
	& \sum_\omega p_\omega = 1 & (\mu) \label{eq:lrlp_primal_probability} \\
	& \x \geq 0 \nonumber \\
	& y^\omega, p_\omega \geq 0\ \forall \omega. \nonumber
\end{align}
Following Wang et.\ al.\ \cite{wang2010likelihood}, we have introduced the likelihood parameter $\gamma$.  The likelihood constraint is equivalent to $\prod_{\omega=1}^n p_\omega^{N_\omega} \geq e^\gamma$.  Let $0 \leq \gamma' \leq 1$ be the relative likelihood parameter that expresses $\gamma$ as a proportion of the maximum likelihood, i.e., $\gamma = \log( \gamma' \prod_\omega (\tfrac{N_\omega}{N})^{N_\omega})$.

Taking the dual of the inner maximization problem, with dual variables $\lambda$ (\ref{eq:lrlp_primal_liklihood}) and $\mu$ (\ref{eq:lrlp_primal_probability}), yields
\begin{align*}
	\min_{\lambda,\mu} \ & \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda(\log\lambda - \log(\mu-\q^\omega y^\omega)) \\
	\st \ & \lambda \geq 0 \\
	& \mu \geq \q^\omega y^\omega \ \forall \omega.
\end{align*}
where $\bar{N} = N(\log N - 1) - \log\gamma'$.  Combining the two minimizations gives LRLP-2 in deterministic equivalent form
\begin{align}
	\min_{\x,\lambda,\mu,y^\omega} \ & \c\x + \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda(\log\lambda - \log(\mu-\q^\omega y^\omega)) \nonumber \\
	\st \ & A\x = \b \nonumber \\
	& -B^\omega \x + D^\omega y^\omega = \d^\omega\ \forall \omega \label{eq:lrlp_det_equiv} \\
	& \mu \geq \q^\omega y^\omega \ \forall \omega \nonumber \\
	& \x,\lambda,y^\omega \geq 0 \ \forall \omega. \nonumber
\end{align}

Finally, we wish to return the LRLP-2 to to two-stage formulation.  All terms inside the sum over $\omega$ will be put into the second stage.  To make the formulation as similar to SLP-2 as possible, we choose to express the second stage as an expected value using the maximum likelihood distribution $\frac{N_\omega}{N}$.  The formulation becomes
\begin{align}
	\min_{\x,\lambda,\mu} \ & \c\x + \mu + \bar{N}\lambda + \sum_\omega \frac{N_\omega}{N} h_\omega(\x,\lambda,\mu) \nonumber \\
	\st \ & A\x = \b \label{eq:lrlp_two_stage} \\
	& \x,\lambda \geq 0, \nonumber
\end{align}
where
\begin{align}
	h_\omega(\x,\lambda,\mu) = \min_{y^\omega} \ & N\lambda (\log\lambda - \log(\mu - \q^\omega y^\omega)) \label{eq:lrlp_second_stage} \\
	\st \ & -B^\omega \x + D^\omega y^\omega = \d^\omega \nonumber \\
	& \mu - \q^\omega y^\omega \geq 0 \label{eq:lrlp_feas_constraint} \\
	& y^\omega \geq 0. \nonumber
\end{align}


Since $\log$ is uniformly increasing, we can rewrite the second stage problem as $h_\omega(\x,\lambda,\mu) = (-N\lambda) \log(\mu - \min_{y^\omega \in Y^\omega} \q^\omega y^\omega )$ with $Y^\omega = \{y^\omega | -B^\omega \x + D^\omega y^\omega = \d^\omega, \mu - \q^\omega y^\omega \geq 0, y^\omega \geq 0\}$.  Thus we can state the second stage of LRLP-2 in terms of the second stage of SLP-2, $h_\omega(\x,\lambda,\mu) = N\lambda\left[\log\lambda - \log(\mu - h_\omega(\x))\right]$.

As noted in \cite{wang2010likelihood}, the KKT conditions for (\ref{eq:lrlp_det_equiv}) give the relation between primal and dual variables
\begin{equation}
	p_\omega = \frac{\lambda N_\omega}{\mu - h^\dagger_\omega(\x)} \label{eq:kkt}
\end{equation}


\section{The Value of Data} \label{sec:value}

With a data driven formulation such as LRLP-2 it is natural to ask how the behavior changes as more data is gathered.  In particular, for ambiguous formulations like LRLP-2 one might be concerned about being overly conservative in the problem formulation and thus missing the opportunity to find a better solution to the true distribution.  We present a simple method of estimating the probability that taking an additional sample will eliminate the old worst-case distribution and allow for better optimization.

Consider again the deterministic equivalent formulation of LRLP-2 (\ref{eq:lrlp_det_equiv}).  Let $f_N(\x,\mu,\lambda) =  \c\x + \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda(\log\lambda - \log(\mu-h^\dagger(\x)))$ be the objective function, and $z_N = \min_{\x,\mu,\lambda} f_N(\x,\mu,\lambda)$.  We wish to find a simple estimate of the decrease in the optimal cost associated with taking an additional sample, $z_N - z_{N+1}$, looking in particular for a condition under which $z_N - z_{N+1} > 0$.

Let $\x^*_N, \mu^*_N,\lambda^*_N \in \argmin f_N(\x,\mu,\lambda)$ be optimal solutions to the $N$-sample problem.  Then $z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N)$ is a lower bound on the decrease in optimal cost $z_N - z_{N+1}$.

Let $\hat{\omega}$ be the scenario that is selected with the additional sample, then $z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N) = \left[ \overline{N} - \overline{N+1} - \log \lambda^*_N + \log(\mu^*_N - h^\dagger_{\hat{\omega}}(\x^*_N)) \right] \lambda^*_N$.  Note that $\lambda^*_N > 0$, so to guarantee a drop in optimal cost we must show that the first term is positive.  To do so, note that $\overline{N} - \overline{N+1} = N\log N - (N+1)\log(N+1) + 1$, which can be bounded using the tangent lines
\[
	\log \x + 1 \leq (\x+1)\log(\x+1) - \x\log \x \leq \log(\x+1) + 1
\]
to get $\overline{N} - \overline{N+1} \geq -\log(N+1)$.

Combining these results gives the condition
\[
	z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N) \geq \left[ -\log(N+1) - \log\lambda^*_N + \log(\mu^*_N-h^\dagger(\x^*_N))\right]\lambda^*_N > 0
\]
which simplifies to
\[
	\frac{\mu^*_N - h^\dagger(\x^*_N)}{\lambda^*_N(N+1)} > 1.
\]
Using the KKT condition (\ref{eq:kkt}), this can be rewritten as
\[
	\frac{N_{\hat{\omega}}}{N} > \left( \frac{N+1}{N} \right) p_{\hat{\omega}}.
\]
That is, we can guarantee a decrease in optimal cost (thus ruling out the previous worst-case distribution) if the probability of the scenario sampled is somewhat more likely to occur in the maximum likelihood distribution than in the worst-case distribution.

Finally, we would like a lower bound on the probability that the next sample will decrease the optimal cost.  Let $L = \{ \omega : \frac{N_{\omega}}{N} \geq \left( \frac{N+1}{N} \right) p_\omega \}$, where $p_\omega$ is the worst-case distribution discussed above.  The lower bound probability can be found by solving the likelihood robust problem
\begin{align}
	\min_{\omega \in L} \ & \sum_{\omega \in L} p_\omega \nonumber \\
	\mbox{s.t.} & \sum_\omega N_\omega \log p_\omega \geq \gamma \label{eq:lb_probability} \\
	& \sum_\omega p_\omega = 1 \nonumber \\
	& p_\omega \geq 0, \ \forall \omega, \nonumber
\end{align}
using parameters identical to the original LRLP-2 (\ref{eq:lrlp_primal}).  We define $N_L = \sum_{\omega \in L} N_\omega$, then $\min_{\omega \in L} \sum_{\omega \in L} p_\omega \leq \frac{N_L}{N}$ by using the maximum likelihood distribution as a feasible solution.  Taking the dual of (\ref{eq:lb_probability}) as before yields the two dimensional minimization
\begin{align}
	\min_{\mu,\lambda} \ & \mu + \bar{N}\lambda + N\lambda\log\lambda - \lambda N_L \log(\mu - 1) - \lambda (N-N_L) \log\mu. \label{eq:prob_cost_decrease}
\end{align}
We view (\ref{eq:prob_cost_decrease}) as a function of parameters $\tfrac{N_L}{N}$,$N$ and $\gamma'$, and plot the optimal value of (\ref{eq:prob_cost_decrease}) as a function of $\tfrac{N_L}{N}$ in Figure \ref{fig:prob_cost_decrease_nd_n} and as a function of $\gamma'$ in Figures \ref{fig:prob_cost_decrease_gp} and \ref{fig:prob_cost_decrease_gp_multiple}.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{images/prob_dec_cost_v_nl_n_20}
	\caption{The probability that an additional sample decreases the optimal cost of the LRLP-2 as a function of the ratio $\frac{N_L}{N}$ for $N = 20$.}
	\label{fig:prob_cost_decrease_nd_n}
\end{figure}

\begin{figure}
	\centering
	\subfloat[][Variance reduction]{
	   \label{fig:prob_cost_decrease_gp}
	   \includegraphics[width=.4\textwidth]{images/prob_dec_cost_v_gammaprime_no_context}}
	\subfloat[][Expected Variance]{
	   \label{fig:prob_cost_decrease_gp_multiple}
	   \includegraphics[width=.4\textwidth]{images/prob_dec_cost_v_gammaprime_no_context_multiple}}
	\caption{The probability that an additional sample decreases the optimal cost of the LRLP-2 as a function of $\gamma'$ for $N = 200$, for a single value of $N_L = 50$ (a) and for multiple values of $N_L$ (b).}
\end{figure}

The plots in Figures \ref{fig:prob_cost_decrease_nd_n}, \ref{fig:prob_cost_decrease_gp} and \ref{fig:prob_cost_decrease_gp_multiple} are presented outside of the context of an LRLP-2.  We note that $\gamma'$ and $N$ are parameters in both LRLP-2 and (\ref{eq:prob_cost_decrease}), but $N_L$ is the result of LRLP-2 optimization, and thus will change with $\gamma'$ in general.  Because $N_L$ is integer, these changes will cause a series of jump discontinuities, as can be seen in Figure \ref{fig:water_prob_decrease}.

\section{Decomposition-Based Solution Method} \label{sec:soln_algorithm}

We propose a Bender's Decomposition-based method for solving LRLP-2.  Our algorithm uses the LRLP-2 second stage problems (\ref{eq:lrlp_second_stage}) to form the cuts, ensuring a linear master problem.  The algorithm requires removing constraint (\ref{eq:lrlp_feas_constraint}) from the second-stage problem (\ref{eq:lrlp_second_stage}) and exchanging it for a series of feasibility constraints in the first-stage problem.  Making this change ensures that the second stage problems are solved using the formulation of $h^\dagger_\theta(\x)$ for SLP-2 (\ref{eq:slp_second_stage}).  The master problem is given by
\begin{align}
	\min_{\x,\lambda,\mu} \ & \c\x + \mu + \bar{N}\lambda + \theta \label{eq:master_problem}\\
	\st \ & A\x = \b \nonumber \\
	& \theta \geq T_j (\x,\lambda,\mu)^T + t_j, \ j \in J \nonumber \\
	& \mu \geq M_k \x + m_k, \ k \in K \nonumber \\
	& \x,\lambda \geq 0, \nonumber
\end{align}
where $T_j (\x,\lambda,\mu)^T + t_j$ are the objective cuts, $M_k \x + m_k$ are the feasibility cuts on constraint (\ref{eq:lrlp_feas_constraint}), and $J$ and $K$ are the sets of objective and feasibility cuts, respectively.

\subsection{Objective Cuts}

Let $(\xh,\lh,\mh)$ be the candidate solution from the master problem (\ref{eq:master_problem}).  An optimality cut can be computed by solving the SLP-2 subproblems $h^\dagger_\omega(\xh)$ along with optimal dual solutions $\pi^{*,\omega}$ to each second stage problem, and using these to compute the partial (sub)derivatives of the LRLP-2 subproblems as
\begin{align*}
	\dfrac{\partial h_\omega}{\partial \mu}(\xh,\lh,\mh) & = \dfrac{-N\lh}{\mh - h_\omega'(\xh)} \\
	\dfrac{\partial h_\omega}{\partial \lambda}(\xh,\lh,\mh) & = N + N\log\lh - N \log(\mh - h_\omega'(\xh)) \\
	\dfrac{\partial h_\omega}{\partial \x}(\xh,\lh,\mh) & = \left(\dfrac{N\lh}{\mh - h_\omega'(\xh)}\right) \pi^{*,\omega} B^\omega
\end{align*}
Recall that $h_\omega'(\x) = \min_{y^\omega \geq 0} \{\q^\omega y^\omega | D^\omega y^\omega = \d^\omega + B^\omega \x\}$.  The cuts are then given by
\begin{align*}
	T_j^\omega & = 
	\left( \begin{array}{ccc}
		\left(\frac{N\lh}{\mh - h_\omega'(\xh)}\right) \pi^{*,\omega} B^\omega, 
			 & N + N\log\lh - N\log(\mh - h_\omega'(\xh)), 
			 & -\frac{N\lh}{\mh - h_\omega'(\xh)}
	\end{array} \right) \\
	t_j^\omega & = \frac{N \lh \mh - N \lh \pi^{*,\omega}B\xh}{\mh - h_\omega'(\xh)}.
\end{align*}
% ---------------------------------------------------
% ---------------------------------------------------
% UPDATE t_j^\omega formula above!!!!!!!
% ---------------------------------------------------
% ---------------------------------------------------

For the single-cut master problem proposed, $T_j = \sum_\omega \frac{N_\omega}{N} T_j^\omega$ and $t_j = \sum_\omega \frac{N_\omega}{N} t_j^\omega$.

\subsection{Feasibility Cuts}
After the subproblems $h_\omega'(\xh)$ are solved, it may be the case that $\mh - h_\omega'(\xh) < 0$ for some $\omega$, rendering $\mh$ infeasible.  This is corrected using the feasibility problem
\begin{align*}
	U_\omega(\x,\mu) = \min_{y_\omega,z \geq 0} \ & z \\
	\st \ & z + \mu - \q^\omega y^\omega \geq 0 \\
	& D^\omega y^\omega = \d^\omega + B^\omega \x,
\end{align*}
which is solved by $z^* = h_\omega'(\x) - \mu$.  The subdifferentials can be easily found as $\frac{\partial z^*}{\partial \x} = \pi^{*,\omega} B^\omega$ and $\frac{\partial z^*}{\partial \mu} = -1$.  Then for infeasible candidate solution $(\xh,\lh,\mh)$ we get the inequality
\begin{align*}
	U_\omega(\x,\mu) \geq \pi^{*,\omega}B^\omega(\x-\xh) - (\mu -\mh) + h_\omega'(\xh) - \mh,
\end{align*}
and setting $U_\omega(\x,\mu) = 0$ to find a feasible solution gives the feasibility cut
\[
	\mu \geq \pi^{*,\omega}B^\omega \x + (h_\omega'(\xh) - \pi^{*,\omega}B^\omega\xh).
\]

Once the feasibility cut is generated, we may need to find a feasible (and reasonable) value of $\mu$ to generate an optimality cut, or to initialize the next iteration of the master problem.  This can be done quickly by minimizing the objective function of (\ref{eq:lrlp_det_equiv}) with respect to $\mu$ while keeping $\xh$ and $\lh$ constant, which is equivalent to minimizing $\mu - \sum_\omega N_\omega \lh \log(\mu - h_\omega'(\xh))$.  We do this by solving the equation $\sum_\omega \frac{N_\omega \lh}{\mu - h_\omega'(\xh)} = 1$ with Newton's method.

\subsection{Decomposition Algorithm}

We solve the LRLP-2 with the following decomposition algorithm with tolerance level $\texttt{TOL}$

\begin{algorithmic}
	\State Initialize $z_l = -\infty, z_u = \infty$
	\State Solve first stage (\ref{eq:master_problem}) with $\theta = 0$  to generate $\x$
	\State Solve all second stage scenarios $h^\dagger_\omega(\x)$ (\ref{eq:slp_second_stage})
	\State Initialize $\lambda \gets 1$, $\mu$ that minimizes $\mu - \sum_\omega N_\omega \lh \log(\mu - h_\omega'(\xh))$
	\State Generate initial objective cut
	\While{$z_u - z_l \geq \texttt{TOL}\min\{|z_u|,|z_l|\}$}
		\State Solve master problem (\ref{eq:master_problem}), get $\x$,$\lambda$,$\mu$,$\theta_M$
		\State Solve sub-problems $h^\dagger_\omega(\x)$ (\ref{eq:slp_second_stage})
		\State $\theta_{\text{true}} \gets \sum_{\omega=1}^n \frac{N_\omega}{N} h_\omega(\x,\lambda,\mu)$
		\If{$\mu < \max_i h_i'(\x)$}
			\State Generate feasibility cut
			\State Find $\mu$ that minimizes $\mu - \sum_\omega N_\omega \lh \log(\mu - h_\omega'(\xh))$
		\Else
			\State $z_l \gets$ master optimal cost $\c\x + \mu + \bar{N}\lambda + \theta_{\text{true}}$
		\EndIf
		\State Generate objective cut
		\If{$\c\x + \mu + \bar{N}\lambda + \theta_{\text{true}} < z_u$}
			\State $z_u \gets \c\x + \mu + \bar{N}\lambda + \theta_{\text{true}}$
			\State $\x_\text{best} \gets \x, \lambda_\text{best} \gets \lambda, \mu_\text{best} \gets \mu$
			\State $p_i \gets \frac{\lambda_\text{best} N_i}{\mu_\text{best} - h^\dagger_i(\x_\text{best})}$ for $i = 1, \dots, n$
		\EndIf
	\EndWhile
\end{algorithmic}

\subsection{Computational Enhancements}

In addition to the above decomposition algorithm, we include some computational enhancements in the algorithm.  We include an $\infty$-norm trust region which is scaled up (by a factor of $3$) or down (by a factor of $\tfrac{1}{4}$) when the trust region inhibits finding the optimal solution or when the polyhedral lower approximation is far from the second-stage expected cost, respectively.  The trust region is an implementation of Algorithm 4.1 in \cite{nocedal1999numerical}.

Because we are also interested in the worst-case probabilities given in the primal variables and not computed directly we include a second tolerance as a stopping condition, ensuring that $\left| 1 - \sum_{i=1}^n p_i \right| < \texttt{TOL2}$ when the algorithm is completed.  This must be satisfied in addition to the original condition $z_u - z_l < \texttt{TOL}\min\{|z_u|,|z_l|\}$

\section{Computational Results} \label{sec:comp_results}

We test LRLP-2 using generalized network model of Colorado River water allocation in the southeastern portion of Tucson of the form (\ref{eq:gen_network_two_stage}).  The model has a total of $P = 41$ time periods, representing years 2010-2050, with projections coming from studies WISP \cite{??} and TAZ \cite{??}.  We use $P_1 = 10$ time periods for the first stage, and four second-stage scenarios are constructed from high (WISP) and low (TAZ) population estimates along with high and low estimates for the amount of water available for use.  Each scenario is assumed to have five observations.  We select tolerances $\texttt{TOL} = 10^{-5}$ and $\texttt{TOL2} = 10^{-3}$ for our computational experiments.

Figure \ref{fig:worst_case} shows how the worst-case distribution changes with $\gamma'$.  The scenarios fall into two similar pairs because the cost of each scenario depends strongly on the projected demand but only weakly on the projected supply of Colorado River water.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{images/worst_case_probability}
	\caption{Worst case distribution for the likelihood robust water allocation problem.}
	\label{fig:worst_case}
\end{figure}


% Upon examining the first-stage solution in it's $10^\text{th}$ and final time period, the advice for planners suggested by the model is clear: plan for the most expensive scenario.  Decisions for water storage for all values of $\gamma'$ are near the values found by the deterministic model using the high-population scenarios.

\subsection{Value of Data}

The results of the water model were then analyzed with the value of data techniques from Section \ref{sec:value}.  Figure \ref{fig:water_prob_decrease} shows the computed values of $\frac{N_L}{N}$ and the estimated probability that an additional sample will remove the worst-case distribution from the likelihood region, analogous the results of Figure \ref{fig:prob_cost_decrease_gp}.  Through most of the computed values of $\gamma'$, we see $\frac{N_L}{N} = 0.5$.  An additional sample of either of the low population scenarios will result in a decreased expected cost.  For extremely large values of $\gamma'$, above $0.97$, we see the ratio $\frac{N_L}{N}$ quickly drop to zero, thus additional samples will not satisfy the sufficient condition for a decrease in objective cost derived in Section \ref{sec:value}.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{images/water_prob_decrease}
	\caption{Probability that an additional sample causes a decrease in worst-case expected cost for the likelihood robust water allocation problem.  The red line shows the upper bound probability $\tfrac{N_L}{N}$.}
	\label{fig:water_prob_decrease}
\end{figure}


\bibliography{love_lro}

\end{document}
