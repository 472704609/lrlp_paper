\documentclass[11pt]{article}

% \usepackage{default}
% \usepackage{fullpage}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}


\bibliographystyle{plain}

\author{David~Love and G\"{u}zin~Bayraksan}

\title{A Likelihood Robust Method for Water Allocation under Uncertainty}
\date{}

% Frequently used general mathematics
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Rp}{\R^+}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Zp}{\Z^+}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}

% Commands for probability
\renewcommand{\P}{\mathbb{P}}
\newcommand{\p}[1]{\P \left[ #1 \right]}
\newcommand{\e}[1]{\mathbb{E} \left[ #1 \right]}
\newcommand{\ee}[2]{\mathbb{E}_{#1} \left[ #2 \right]}

% Definitions of variables
\newcommand{\X}{X}
\newcommand{\x}{\mathbf{x}}
\newcommand{\xh}{\hat{\x}}
\newcommand{\lh}{\hat{\lambda}}
\newcommand{\mh}{\hat{\mu}}
\newcommand{\xs}{\x^*}
\newcommand{\xit}{\tilde{\mathbf{\xi}}}
\newcommand{\zt}{\tilde{z}}
\newcommand{\zs}{z^*}

% Further variables
\newcommand{\y}{\mathbf{y}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\q}{\mathbf{q}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\d}{\mathbf{d}}

% Epiconvergence for LRLP-2
\newcommand{\ptrue}{p^{\text{true}}}

% Useful mathematics functions
\newcommand{\keywords}[1]{\par\noindent\enspace\ignorespaces\textbf{Keywords:} #1}
% \newcommand{\keywords}[1]{\par\addvspace\baselineskip\noindent\keywordname\enspace\ignorespaces #1}
\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newcommand{\st}{\mbox{s.t.}}

\begin{document}

\maketitle

\begin{abstract}
	We adapt and extend the likelihood robust optimization method recently proposed by Wang, Glynn, and Ye to examine water allocation under the ambiguous distribution of future available supply and demand in a two-stage setting.
	We examine the value of collecting additional data and the cost of finding a solution robust to an ambiguous probability distribution.
	A decomposition-based solution algorithm to solve the resulting model is given.
	Computational results are presented on a long-term water allocation problem in southeast area of Tucson.
\end{abstract}

\keywords{Optimization under uncertainty, water resources management,  ambiguous stochastic programming, robust optimization, environmental sustainability}

\section{Introduction and Motivation}

More than 25 million people in the southwestern United States depend on the water supplied by the Lower Colorado River Basin for their livelihood.
More than half of Tucson's water, for instance, comes from this source.
The Colorado River Basin has experienced a sustained period of drought in recent years, which has led to questions about the adequacy of the Colorado to meet future demands, especially as the population of Arizona (and of other states that depend on this water source) increases.
Thus, the problem of allocating Colorado water is of critical importance. 
 
The reliability of the Colorado River system under future climate variability is critically important to the long-term well-being of Arizona and the other six states that depend on this water supply \cite{usbr_colorado_climate}. 
The current approach to modeling the water supply is to use Global Circulation Models (GCM) to generate regional rainfall and temperature scenarios by the so-called statistical downscaling techniques \cite{christensen_lettenmaier_07,dibike_caulibaly_05}.  
The US Bureau of Reclamation also uses an approach called the {\it scenario planning} that examines water demand and supplies over the next 50 years \cite{usbr_11}.

In this paper, we present a generalized network model of Colorado River water allocation in Tucson, Arizona motivated by the CALVIN (CALifornia Value Integrated Network) optimal water allocation model of California created by Draper et al.\ \cite{draper_etal_03}.
More general models of Colorado River water distribution have also been studied, such as the Colorado River Reservoir Model \cite{christensen2004effects} and the Colorado River Budget Model \cite{barnett2009sustainable}.
Our model is modified to incorporate future uncertainty by using the Likelihood Robust Optimization (LRO) approach of Wang, Glynn and Ye \cite{wang2010likelihood}.
LRO is a data-driven method that uses observations of random or unknown parameters to account not only for inherent stochasticity of a problem, but the uncertainty of the probabilistic model itself.
This is especially important in our application as we are looking 40 years into the future and there is considerable ambiguity in the uncertainties.

The LRO is especially attractive because only those scenarios of interest, obtained either through observation or simulation, are used directly in the calculations.
Thus the size of the problem is polynomial in the sample size, making it computationally tractable.
Furthermore, the samples used in the LRO can represent select scenarios that are of interest to the authorities---for instance, generated by the scenario planning process \cite{cityofTucsonWaterPlan,usbr_11}.
This fits with practice well; the scenario planning process results in several scenarios that the agencies would especially like to be robust against.

Distributionally robust methods like LRO have been drawing considerable attention in recent years.
Erdo{\u{g}}an and Iyengar \cite{erdogan2006ambiguous} studied chance-constrained stochastic program where the set of distributions considered is determined by the Prohorov metric.
Calafiore and Campi \cite{calafiore2005uncertain} developed a data-driven method for generating feasible solutions to chance constrained problems, and later Calafiore and El Ghaoui \cite{calafiore2006distributionally} developed a method for converting distributionally robust chance constraints into second-order cone constraints.
Jiang and Guan \cite{jiang2012data} developed an exact approach to solving data-driven chance constrained programs.

Stochastic programs with uncertain objective functions have long been studied applying the minimax approach to an expected cost \cite{dupacova_87}.
Shapiro and Kleywegt \cite{shapiro2002minimax} and Shapiro and Ahmed \cite{shapiro2004class} developed methods for converting stochastic minimax problems into equivalent stochastic programs with a certain distribution.
Delage and Ye \cite{delage_ye_10} provide methods for modeling uncertain distributions of a specific form (e.g., Gaussian, exponential, etc.) or using moment-based constraints.
Ben-Tal \cite{bental2011robust} studies distributionally robust stochastic programs when the uncertainty region is defined by selecting distributions using a $\phi$-divergence.

The contributions of this paper are as follows
\begin{enumerate}
	\item Apply the LRO methodology to a more general two-stage stochastic linear program with recourse.
	\item Develop a modified Bender's Decomposition to solve the resulting LRO model.
	\item Develop a simple condition for determining the value of collecting additional data.
	\item Apply the above results to a water distribution planning problem.
\end{enumerate}

This paper is organized as follows: in Section \ref{sec:network_model} we state the generalized network model in deterministic and stochastic forms; in Section \ref{sec:lrlp2} we extend the Likelihood Robust Optimization (LRO) model for a two-stage stochastic program with recourse; in Section \ref{sec:value} we discuss a method of measuring the value of gathering additional data for input to the LRO; in Section \ref{sec:soln_algorithm} we present a decomposition method for solving the LRO model; and in Section \ref{sec:comp_results} we present some computational results for our application. Finally, we end in Section \ref{sec:concl} with conclusions and future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized Network Water Model} 
\label{sec:network_model}

We begin with a multi-period generalized network flow model of Colorado River water allocation in Tucson, defined by a set of nodes and directed arcs $(N,\: A)$.
The nodes represent available water supply from the Colorado River, water treatment plants, reservoirs, and water demand sites.
The arcs represent the conveyance system (pipes, etc.) that carry water between the nodes. 
Water can be stored in between time periods in reservoirs to meet future demands. 
The model aims to find the minimal cost water flows considering energy, treatment, storage, and transportation costs over the planning period. 
Generalized network water allocation models have been used to find water allocations and delivery reliabilities and to assess values of different water use operations; see, e.g., \cite{draper_etal_03}. 

Water flows on arc $(i,j) \in A$ during time period $t = 1, \dots, P$ are represented by decisions $x_{ijt}$.
Each arc $(i,j) \in A$ and time period $t$ has a unit cost $c_{ijt}^x$, loss coefficient $0 \leq a_{ijt} \leq 1$ to account for evaporation, leakage from the pipes, etc., and bounds on the flow $l_{ijt}^x \leq x_{ijt} \leq u_{ijt}^x$.
Each node $j \in N$ has a supply/demand for time period $t$, $b_{jt}$.
Nodes representing reservoirs are able to store water between time periods.
Stored water available at node $j$ at the beginning of time period $t$ is $s_{jt}$, with associated cost $c_{jt}^s$ and bounds $l_{jt}^s \leq s_{jt} \leq u_{jt}^s$.
Finally, water released into the environment from node $j$ in period $t$ is given by $r_{jt}$, with bounds $l_{jt}^r \leq r_{jt} \leq u_{jt}^r$.
The deterministic model is a multi-period generalized network flow model of the form
\begin{align*}
	\min_{x,s,r} \ & \sum_{(i,j) \in A} \sum_{t=1}^P c_{ijt}^x x_{ijt} + \sum_{j \in N} \sum_{t=1}^P c_{jt}^s s_{jt}\\
	\st \ & \sum_{i \in N} x_{jit} + s_{j,t+1} + r_{jt} = \sum_{i \in N} a_{ijt} x_{ijt} + s_{jt} + b_{jt}, \ \ \forall j,t \\
	& l_{ijt}^x \leq x_{ijt} \leq u_{ijt}^x,\ \ \ \forall i,j,t \\
	& l_{jt}^s \leq s_{jt} \leq u_{sjt}^s, \ \ \ \forall j,t \\
	& l_{jt}^r \leq r_{jt} \leq u_{sjt}^r, \ \ \ \forall j,t.
\end{align*}


In practice, many parameters are unknown, especially as we further look into the future. 
This is particularly true of the water supply and demands, denoted by $b_{jt}$ in the above model. 
To capture this uncertainty, we make this model stochastic by rewriting it as a two-stage linear program.
The model has a total of $P$ time periods, which is split into two stages of $P_1$ and $P_2 = P - P_1$ years each. 
The water supplies and demands $b_{jt}$ are assumed to be known during the first stage of $P_1$ years and in the remaining $P_2$ years, they are uncertain and modeled using a discrete set of scenarios with a specific probability distribution.  
Note that we also assume some lower and upper bounds to be stochastic in the second stage. 
The model thus becomes
\begin{align}
	\min_{(x,s,r) \in L^1} \ & \sum_{(i,j) \in A} \sum_{t=1}^{P_1} c_{ijt}^x x_{ijt} + \sum_{j \in N} \sum_{t=1}^{P_1} c_{jt}^s s_{jt} + \sum_{\omega=1}^n p_\omega h^\dagger_\omega(s) \label{eq:gen_network_two_stage} \\
	\st \ & \sum_{i \in N} x_{jit} + s_{j,t+1} + r_{jt} = \sum_{i \in N} a_{ijt} x_{ijt} + s_{jt} + b_{jt},\ \ \ \ \forall j, 1 \leq t \leq P_1, \notag
\end{align}
where the $\omega$ index indicates the $n$ second-stage scenarios with probabilities $p_\omega$,
\begin{align}
	h^\dagger_\omega(s) = \min_{(x,s,r) \in L^2_\omega} \ & \sum_{(i,j) \in A} \sum_{t=P_1+1}^{P} c_{ijt}^x x_{ijt} + \sum_{j \in N} \sum_{t=P_1+1}^{P} c_{jt}^s s_{jt} \label{eq:gen_network_second_stage} \\
	\st \ & \sum_{i \in N} x_{jit} + s_{j,t+1} + r_{jt} = \sum_{i \in N} a_{ijt} x_{ijt} + s_{jt} + b_{jt}^\omega, \ \ \ \ \forall j, P_1+1 \leq t \leq P, \notag
\end{align}
and $L^1$ and $L^2_\omega$ represent the feasible regions defined by the lower and upper variable bounds.

In the rest of the paper, we simplify the notation for the first-stage (\ref{eq:gen_network_two_stage}) and second-stage (\ref{eq:gen_network_second_stage}) problems as follows.
In the first stage, decision variables $\{x_{ijt}\}$, $\{s_{jt}\}$ and $\{r_{jt}\}$ become the vector $\x$, costs $\{c_{ijt}^x\}$ and $\{c_{jt}^s\}$ are written as the row vector $\c$, the supply/demand parameters $b_{jt}$ become the vector $\b$ and the constraint matrix is written as $A$.
In the second stage, we denote the decisions as $\y^\omega$, the costs as $\q^\omega$, the supply/demands as $\d^\omega$, and the constraint matrices multiplying $\y^\omega$ and $\x$ as $D^\omega$ and $B^\omega$, respectively.
This notation puts the generalized network model in the form of a two-stage stochastic linear program with recourse (SLP-2), formulated as
\begin{align}
	\min_\x \ & \c\x + \sum_{\omega=1}^n p_\omega h^\dagger(\x) \label{eq:slp_first_stage} \\
	\st \ & A\x = \b \nonumber  \\
	&\ \ \ \x \geq 0 \nonumber
\end{align}
where
\begin{align}
	h^\dagger_\omega(\x) = \min_{y^\omega} \ & \q^\omega y^\omega \label{eq:slp_second_stage} \\
	\st \ & D^\omega y^\omega = B^\omega \x + \d^\omega \nonumber \\
	& \ \ \ y^\omega \geq 0. \nonumber
\end{align}
For simplicity we assume relatively complete recourse; i.e., the second-stage problems $h^\dagger_\omega(\x)$ are feasible for every feasible solution $\x$ of the first-stage problem. In our application, there are penalty costs when demand is not met; therefore, our model has relatively complete recourse. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LRLP-2 Formulation}
\label{sec:lrlp2}

The SLP-2 formulation assumes that the distribution $\{p_\omega\}$ is known.
However, in many applications, including our water planning, the distribution is itself unknown.
One technique to deal with this is to replace the known distribution with an {\it ambiguity set} of distributions; i.e., a set of distributions which is believed to contain the true distribution.
In the likelihood robust formulation, we assume scenario $\omega$ has been observed $N_\omega$ times, with $N = \sum_{\omega=1}^n N_\omega$ total observations. 
In SLP-2, this would correspond to probability of scenario $\omega$ to be $p_\omega = N_\omega / N$, which is the maximum likelihood distribution. 
The {\it ambiguity set}, however, is defined by the set of distributions with sufficiently high empirical likelihood $\prod_{\omega=1}^n p_\omega^{N_\omega}$. 
By replacing the specific distribution in SLP-2 with a set of distributions with  high empirical likelihood, we create a model that we refer to as two-stage likelihood robust linear program with recourse (LRLP-2).

To derive the LRLP-2, we begin by writing SLP-2 given in \eqref{eq:slp_first_stage}--\eqref{eq:slp_second_stage} in extensive form
\[
	\begin{array}{rrrl}
		\min_{\x,y^\omega} \ & \c\x & + \sum_\omega p_\omega \q^\omega y^\omega \label{eq:slp2cost} \\
		\st \ & A\x & & = \b \nonumber \\
		& -B^\omega \x & + D^\omega y^\omega & = \d^\omega,\ \forall \omega \nonumber \\
		& \x & & \geq 0 \nonumber \\
		& & y^\omega & \geq 0,\ \forall \omega. \nonumber
	\end{array}
\]
The SLP-2 formulation is then augmented by the set of distributions with sufficiently high likelihood. 
%selecting the worst-case distribution $p_\omega$ over the set of distributions with sufficiently high likelihood.
%That is, instead of using a specific probability distribution as in SLP-2---for instance, by maximum likelihood estimation that results in $p_\omega = \frac{N_\omega}{N}$, $\forall \omega$---all distributions that have sufficiently high empirical likelihood values are considered.
To be robust against all these possible distributions, the distribution that results in the maximum expected cost is considered. 
Then, the objective function is minimized with respect to this worst-case distribution selected from the ambiguity set of distributions.
The resulting minimax formulation of LRLP-2 is
\begin{align}
	\min_{\x,y^\omega} \max_p \ & \c\x + \sum_\omega p_\omega \q^\omega y^\omega \label{eq:lrlp_primal}\\
	\st \ & A\x = \b \nonumber \\
	& -B^\omega \x + D^\omega y^\omega = \d^\omega,\ \forall \omega \nonumber \\
	& \sum_\omega N_\omega \log p_\omega \geq \gamma & (\lambda) \label{eq:lrlp_primal_liklihood} \\
	& \sum_\omega p_\omega = 1 & (\mu) \label{eq:lrlp_primal_probability} \\
	& \x \geq 0 \nonumber \\
	& y^\omega, p_\omega \geq 0,\ \forall \omega. \nonumber
\end{align}
Following Wang et al.\ \cite{wang2010likelihood}, we have introduced the likelihood parameter $\gamma$, and used it to construct the ambiguity set of distributions $\{p_\omega\}$ satisfying constraints (\ref{eq:lrlp_primal_liklihood}) and (\ref{eq:lrlp_primal_probability}).
Note the likelihood constraint \eqref{eq:lrlp_primal_liklihood} is equivalent to $\prod_{\omega=1}^n p_\omega^{N_\omega} \geq e^\gamma$, which explicitly states that the empirical likelihood should be above a certain desired level dictated by $\gamma$. 
Constraint \eqref{eq:lrlp_primal_probability}, along with nonnegativity constraints on $p_\omega$, simply ensures that $\{p_\omega\}$ constitutes a probability distribution. 
Let $0 \leq \gamma' \leq 1$ be the \emph{relative likelihood parameter} that expresses $\gamma$ as a {\it proportion} of the maximum likelihood; i.e., $\gamma = \log( \gamma' \prod_\omega (\tfrac{N_\omega}{N})^{N_\omega})$.
In the rest of the paper, we will be referring to $\gamma'$. 


Taking the dual of the inner maximization problem, with dual variables $\lambda$ and $\mu$, of constraints (\ref{eq:lrlp_primal_liklihood}) and (\ref{eq:lrlp_primal_probability}), respectively, yields
\begin{align*}
	\min_{\lambda,\mu} \ & \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda(\log\lambda - \log(\mu-\q^\omega y^\omega)) \\
	\st \ & \lambda \geq 0 \\
	& \mu \geq \q^\omega y^\omega, \ \forall \omega,
\end{align*}
where $\bar{N} = N(\log N - 1) - \log\gamma'$.
Combining the two minimizations gives LRLP-2 in extensive form
\begin{align}
	\min_{\x,\lambda,\mu,y^\omega} \ & \c\x + \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda(\log\lambda - \log(\mu-\q^\omega y^\omega)) \nonumber \\
	\st \ & A\x = \b \nonumber \\
	& -B^\omega \x + D^\omega y^\omega = \d^\omega,\ \forall \omega \label{eq:lrlp_det_equiv} \\
	& \mu \geq \q^\omega y^\omega, \ \forall \omega \nonumber \\
	& \x,\lambda,y^\omega \geq 0, \ \forall \omega. \nonumber
\end{align}

Finally, we wish to return the LRLP-2 to two-stage formulation.
All terms inside the sum over $\omega$ will be put into the second stage.
To make the formulation as similar to SLP-2 as possible, we choose to express the second stage as an expected value using the maximum likelihood distribution $\frac{N_\omega}{N}$.
The formulation becomes
\begin{align}
	\min_{\x,\lambda,\mu} \ & \c\x + \mu + \bar{N}\lambda + \sum_\omega \frac{N_\omega}{N} h_\omega(\x,\lambda,\mu) \nonumber \\
	\st \ & A\x = \b \label{eq:lrlp_two_stage} \\
	& \x,\lambda \geq 0, \nonumber
\end{align}
where
\begin{align}
	h_\omega(\x,\lambda,\mu) = \min_{y^\omega} \ & N\lambda (\log\lambda - \log(\mu - \q^\omega y^\omega)) \label{eq:lrlp_second_stage} \\
	\st \ & -B^\omega \x + D^\omega y^\omega = \d^\omega \nonumber \\
	& \mu - \q^\omega y^\omega \geq 0 \label{eq:lrlp_feas_constraint} \\
	& y^\omega \geq 0. \nonumber
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Properties of LRLP-2}

\subsection{Coherent Risk Measure}
As noted in \cite{wang2010likelihood}, the LRLP-2 problem can be viewed as minimizing a coherent risk measure.
A coherent risk measure (in the basic sense), as defined in \cite{rockafellar2007coherent}, is a functional ${\cal R}: L^2 \rightarrow (-\infty,\infty]$ defined on random variables such that
\begin{enumerate}
	\item ${\cal R}(C) = C$ for all constants $C$,
	\item ${\cal R}((1-\lambda)X + \lambda X') \leq (1-\lambda){\cal R}(X) + \lambda {\cal R}(X')$, i.e., $\cal R$ is convex,
	\item ${\cal R}(X) \leq {\cal R}(X')$ when $X \leq X'$, i.e., $\cal R$ is monotonic,
	\item ${\cal R}(\lambda X) = \lambda {\cal R}(X)$, i.e., $\cal R$ is positively homogeneous.
\end{enumerate}

\begin{proposition}
	LRLP-2 is equivalent to minimizing a coherent risk measure.
\end{proposition}

\begin{proof}
	Rockafellar also shows that $\cal R$ is a coherent risk measure if and only if it can be written using a risk envelope \cite{rockafellar2007coherent}.
	We will show that LRLP-2 can be written in the form of a risk envelope in the primal form (\ref{eq:lrlp_primal}) with the change of variables $Q_\omega = \frac{p_\omega}{1/n}$.
	Throughout the proof, all expectations are taken with respect to the uniform distribution.
	
	First, probability constraint (\ref{eq:lrlp_primal_probability}) can be written as $\e{Q} = 1$, where $Q$ is the random variable taking values $Q_\omega$ with equal probability.
	Then the likelihood constraint (\ref{eq:lrlp_primal_liklihood}) becomes $\sum_{\omega=1}^n N_\omega \log Q_\omega \geq \gamma - N \log n$.
	Combining these yields the set ${\cal Q} = \{Q | \e{Q} = 1, \sum_{\omega=1}^n N_\omega \log Q_\omega \geq \gamma - N \log n \}$, a closed and convex risk envelope.
	Finally, we can rewrite the inner maximization of (\ref{eq:lrlp_primal}) as $\max_{Q \in {\cal Q}} \e{Qh^\dagger(x)}$, where $h^\dagger(x)$ is the random variables defined by $\{h^\dagger_\omega(x)\}$.
	Thus we see that LRLP-2 is the minimum of a coherent risk measure.
\end{proof}

Note that being a coherent risk measure implies that LRLP-2 is a convex problem.
The convexity of LRO was also noted in \cite{wang2010likelihood}.

\subsection{Time Structure}
LRLP-2 has the same second-stage problems as SLP-2.
Since $\log$ is uniformly increasing, we can rewrite the second stage problem (\ref{eq:lrlp_second_stage}) as $h_\omega(\x,\lambda,\mu) = (-N\lambda) \log(\mu - \min_{y^\omega \in Y^\omega} \q^\omega y^\omega )$ with $Y^\omega = \{y^\omega | -B^\omega \x + D^\omega y^\omega = \d^\omega, \mu - \q^\omega y^\omega \geq 0, y^\omega \geq 0\}$.
Thus we can state the second stage of LRLP-2 in terms of the second stage of SLP-2, $h_\omega(\x,\lambda,\mu) = N\lambda\left[\log\lambda - \log(\mu - h^\dagger_\omega(\x))\right]$.

This preservation of the time structure allows us to easily convert (sub-)derivatives of $h^\dagger_\omega(\x)$ to (sub-)derivatives of $h_\omega(\x,\lambda,\mu)$.
We will use this in the decomposition method provided in Section \ref{sec:soln_algorithm}.

\subsection{KKT Conditions}
As noted in \cite{wang2010likelihood}, the KKT conditions for (\ref{eq:lrlp_det_equiv}) give the relation between primal and dual variables
\begin{equation}
	p_\omega = \frac{\lambda N_\omega}{\mu - h^\dagger_\omega(\x)}. \label{eq:kkt}
\end{equation}

\subsection{Relation to KL Divergence}
The likelihood condition (\ref{eq:lrlp_primal_liklihood}) can be written in the form of a Kullback-Leibler divergence condition, i.e., (\ref{eq:lrlp_primal_liklihood}) can be rewritten as
\[
	\sum_{\omega=1}^n \frac{N_\omega}{N} \log\left( \frac{N_\omega/N}{p_\omega} \right) \leq \frac{1}{N} \log \frac{1}{\gamma'},
\]
where $\sum_{\omega=1}^n q_\omega \log \frac{q_\omega}{p_\omega}$ is the KL divergence of the distributions $D_{KL}(q,p)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Level of Robustness}

We have introduced the relative likelihood parameter $\gamma'$, but have not yet made any recommendations on how to choose it.
Remark 3.1 of \cite{pardo2005statistical} shows that $2ND_{KL}(\hat{p}^N, \ptrue) \Rightarrow \chi^2_{n-1}$, where $\ptrue$ is the true distribution from which the empirical distribution $\hat{p}^N$ is sampled, $\Rightarrow$ indicates convergence in distribution, and $\chi^2_{n-1}$ is a $\chi^2$ distribution with $n-1$ degrees of freedom.
Thus, if we want to choose $\gamma'$ to generate a $100(1-\alpha)\%$ confidence region on the true distribution, select the asymptotic value
\begin{equation} \label{eq:asymptotic_gamma}
	\gamma' = \exp \left( -\frac{\chi^2_{n-1,1-\alpha}}{2} \right).
\end{equation}
Note that $\gamma'$ constant asymptotically produces constant-size confidence regions.

Wang et. al. \cite{wang2010likelihood} suggest a Bayesian interpretation of the likelihood constraint (\ref{eq:lrlp_primal_liklihood}) which yields a Monte Carlo method of establishing a value for $\gamma'$.
Considering the observations $\{N_\omega\}$ and empirical probabilities $\hat{p}^N_\omega = \frac{N_\omega}{N}$ as fixed, Wang et. al. \cite{wang2010likelihood} introduce a distribution $\P^*$ on the $n$-dimensional simplex whose density is proportional to the likelihood $\prod_\omega p_\omega^{N_\omega}$.
This is the Dirichlet distribution $Dir(\beta)$, which has density
\[
	\frac{1}{B(\beta)} \prod_{\omega=1}^n p_\omega^{\beta_\omega-1}
\]
where the normalizing factor is $B(\beta) = \frac{\prod_{\omega=1}^n \Gamma(\beta_\omega)}{\Gamma\left( \sum_{\omega=1}^n \beta_\omega \right)}$.
The unknown true distribution can then be modeled as coming from a $Dir(\beta)$ distribution with parameters $\beta_\omega = N_\omega + 1$.
The parameter $\gamma'$ is chosen such that $\P^*\left\{ \prod_{\omega=1}^n p_\omega^{N_\omega} \geq \gamma' \prod_{\omega=1}^n (\hat{p}^N_\omega)^{N_\omega} \right\}$.
This probability can be estimated by Monte Carlo sampling.

% Figure \ref{fig:gammaprime_by_sample} shows the estimated values of $\gamma'$ for a $95\%$ confidence region for distributions with $n = 10$ scenarios.
% You can notice four distinct, color-coded ``bands'' of values of $\gamma'$.
% These bands are determine by the size of the set $\{\omega | N_\omega = 0\}$, i.e., the number of scenarios which have not been sampled.
% The highest band, in blue, has all $N_\omega > 0$.
% The next (green) band has one $N_\omega = 0$, followed by red with two and cyan with three.
% 
% \begin{figure}
% 	\centering
% 	\includegraphics[width=.6\textwidth]{images/gammaprime_diff_samples_dim_10}
% 	\caption{
% 		Computed estimates for $\gamma'$ to achieve a $95\%$ confidence region using the Dirichlet distribution.
% 		Each point estimate was computed using a randomly generated $n=10$ dimensional probability distribution from which $N$ samples were selected.
% 		The value of $\gamma'$ was estimated by Monte Carlo sampling from $Dir(\beta)$ with $\beta_\omega = N_\omega + 1$.
% 		The color indicates how many of the $n$ scenarios were not represented in the $N$ samples: blue indicates that all $N_\omega > 0$, green has exactly one $N_\omega = 0$, red has two, and cyan has three.
% 	}
% 	\label{fig:gammaprime_by_sample}
% \end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Value of Data} \label{sec:value}

With a data driven formulation such as LRLP-2, it is natural to ask how the behavior changes as more data is gathered.
In particular, for robust formulations like LRLP-2 one might be concerned about being overly conservative in the problem formulation and thus missing the opportunity to find a better solution to the true distribution.
For instance, for our water application, initial estimates might show that water demand would be high or low with approximately equal probabilities. 
Based on new data, later studies could give low demand an increased weight.  
For LRLP-2, this means that the initial model is likely to be more conservative in an effort to be robust, while the new information will make the model less conservative because low water demand results in lower costs and our belief is increased that the future demand will be low.  
In this section, we present a simple method of estimating the probability that taking an additional sample will eliminate the old worst-case distribution and allow for better optimization; i.e., a lower-cost solution.

\begin{proposition}
	An additional sample of scenario $\hat{\omega}$ will result in a decrease in the worst-case expected cost of the LRLP-2 if the following condition is satisfied
	\begin{equation} \label{eq:cost_decrease_cond}
		\frac{N_{\hat{\omega}}}{N} > \left( \frac{N+1}{N} \right) p_{\hat{\omega}},
	\end{equation}
	where $p_{\hat{\omega}}$ is the probability of scenario $\hat{\omega}$ in the worst-case distribution.
\end{proposition}

\begin{proof}
	Consider again the deterministic equivalent formulation of LRLP-2 in (\ref{eq:lrlp_det_equiv}).
	Let $f_N(\x,\mu,\lambda) =  \c\x + \mu + \bar{N}\lambda + \sum_\omega N_\omega\lambda$ $(\log\lambda$ $- \log(\mu-h^\dagger(\x)))$ be the objective function, and $z_N = \min_{\x,\mu,\lambda} f_N(\x,\mu,\lambda)$.
	We wish to find a simple estimate of the decrease in the optimal cost associated with taking an additional sample, $z_N - z_{N+1}$, looking in particular for a condition under which $z_N - z_{N+1} > 0$.

	Let $\x^*_N, \mu^*_N,\lambda^*_N \in \argmin f_N(\x,\mu,\lambda)$ be optimal solutions to the $N$-sample problem.
	Then $z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N)$ is a lower bound on the decrease in optimal cost $z_N - z_{N+1}$.
	Let $\hat{\omega}$ be the scenario that is selected with the additional sample, then
	\[
		z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N) = \left[ \overline{N} - \overline{N+1} - \log \lambda^*_N + \log(\mu^*_N - h^\dagger_{\hat{\omega}}(\x^*_N)) \right] \lambda^*_N.
	\]
	We can bound $\overline{N} - \overline{N+1} = N\log N - (N+1)\log(N+1) + 1$ by using the tangent lines
	\[
		\log \x + 1 \leq (\x+1)\log(\x+1) - \x\log \x \leq \log(\x+1) + 1
	\]
	to get $\overline{N} - \overline{N+1} \geq -\log(N+1)$.

	Combining these results gives the condition
	\[
		z_N - f_{N+1}(\x^*_N,\mu^*_N,\lambda^*_N) \geq \left[ -\log(N+1) - \log\lambda^*_N + \log(\mu^*_N-h^\dagger(\x^*_N))\right]\lambda^*_N > 0.
	\]
	Note that $\lambda^*_N > 0$, so to guarantee a drop in optimal cost we must show that the first term is positive.
	This then simplifies to
	\[
		\frac{\mu^*_N - h^\dagger(\x^*_N)}{\lambda^*_N(N+1)} > 1.
	\]
	Using the KKT condition (\ref{eq:kkt}), this can be rewritten as (\ref{eq:cost_decrease_cond}).
\end{proof}

We can interpret \eqref{eq:cost_decrease_cond} as follows. If an additional sample is taken from the unknown distribution and the resulting observed scenario $\hat{\omega}$ satisfies (\ref{eq:cost_decrease_cond}), then the $(N+1)$-sample problem will have a lower cost than the $N$-sample problem that was already solved.
This is equivalent to saying that an additional observation of $\hat{\omega}$ will rule out the computed worst-case distribution given by $\{p_\omega\}$ given in \eqref{eq:kkt}.

% Finally, we would like a lower bound on the probability that the next sample will decrease the optimal cost.
% Let $L = \left\{ \omega : \frac{N_{\omega}}{N} > \left( \frac{N+1}{N} \right) p_\omega \right\}$, where $p_\omega$ is the worst-case distribution discussed above.
% That is, $L$ gives the set of scenarios that, if sampled one more observation, would result in a decrease in the optimal cost in LRLP-2.
% 
% \begin{proposition}
% 	An approximate lower bound on the probability of selecting a scenario in the set $L= \left\{ \omega : \frac{N_{\omega}}{N} > \left( \frac{N+1}{N} \right) p_\omega \right\}$ can be found by solving 
% 	\begin{equation} \label{eq:prob_cost_decrease}
% 		-\min_{\mu \geq 0,\lambda \geq 0} \mu + \bar{N}\lambda + N\lambda\log\lambda - \lambda N_L \log(\mu + 1) - \lambda (N-N_L) \log\mu,
% 	\end{equation}
% 	where $N_L = \sum_{\omega \in L} N_\omega$ be the number of observations in set $L$.
% \end{proposition}
% 
% \begin{proof}
% 	We can estimate a lower bound on the probability of sampling a scenario in $L$ by using the same likelihood ambiguity set that was used to formulate LRLP-2 given in (\ref{eq:lrlp_primal_liklihood}) to solve the minimization problem
% 	\begin{align}
% 		\min_{\omega \in L} \ & \sum_{\omega \in L} q_\omega \nonumber \\
% 		\mbox{s.t.} & \sum_\omega N_\omega \log q_\omega \geq \gamma \label{eq:lb_probability} \\
% 		& \sum_\omega q_\omega = 1 \nonumber \\
% 		& q_\omega \geq 0, \ \forall \omega, \nonumber
% 	\end{align}
% 	where we have introduced the dummy variables $q_\omega$ to distinguish the minimization in (\ref{eq:lb_probability}) from the worst-case distribution $\{p_\omega\}$ calculated in (\ref{eq:kkt}). 
% 	Solving (\ref{eq:lb_probability}) yields an estimated lower bound on the probability that an additional sample will result a likelihood ambiguity set that does not contain the current worst-case distribution $\{p_\omega\}$ using the current set of observations.
% 	Note that $\min_{\omega \in L} \sum_{\omega \in L} q_\omega \leq \frac{N_L}{N}$, because the maximum likelihood distribution is always within the likelihood ambiguity set. We will use $\frac{N_L}{N}$ as a benchmark in Figures~\ref{fig:prob_cost_decrease_nd_n}, \ref{fig:prob_cost_decrease_gp} and \ref{fig:water_prob_decrease}. 
% 
% 	We solve (\ref{eq:lb_probability}) by taking its dual, which results in the two dimensional nonlinear program (\ref{eq:prob_cost_decrease}).
% \end{proof}
% 
% We can view the optimal value of (\ref{eq:prob_cost_decrease}) as a function of three parameters: the total sample size $N$, the relative likelihood parameter $\gamma'$, and the number of observations $N_L$ in the set $L$.
% The behavior of this lower bound estimate is studied in Figure \ref{fig:prob_cost_decrease_nd_n} as a function of the ratio $\tfrac{N_L}{N}$, and in Figures \ref{fig:prob_cost_decrease_gp} and \ref{fig:prob_cost_decrease_gp_multiple} as a function of the relative likelihood parameter $\gamma'$.
% Figure \ref{fig:prob_cost_decrease_nd_n} shows that the estimated lower bound on the probability of optimal cost decrease stays relatively close to the identity line for most values of $\gamma'$, getting closer to the identity line as  $\gamma'$ is increased; i.e., the ambiguity set (or robustness) is decreased.
% Figures \ref{fig:prob_cost_decrease_gp} and \ref{fig:prob_cost_decrease_gp_multiple} give a closer look at how (\ref{eq:prob_cost_decrease}) differs from $\tfrac{N_L}{N}$ as $\gamma'$ is changed.
% 
% \begin{figure}
% 	\centering
% 	\includegraphics[width=.5\textwidth]{images/prob_dec_cost_v_nl_n_20}
% 	\caption{The probability that an additional sample decreases the optimal cost of the LRLP-2 as a function of the ratio $\frac{N_L}{N}$ for total sample size $N = 20$.}
% 	\label{fig:prob_cost_decrease_nd_n}
% \end{figure}
% 
% \begin{figure}
% 	\centering
% 	\subfloat[][$N_L=50$ $\left(\frac{N_L}{N} = 0.25\right)$]{
% 	   \label{fig:prob_cost_decrease_gp}
% 	   \includegraphics[width=.4\textwidth]{images/prob_dec_cost_v_gammaprime_no_context}}
% 	\subfloat[][$N_L=10,50,100,150$ and $190$]{
% 	   \label{fig:prob_cost_decrease_gp_multiple}
% 	   \includegraphics[width=.4\textwidth]{images/prob_dec_cost_v_gammaprime_no_context_multiple}}
% 	\caption{The probability that an additional sample decreases the optimal cost of the LRLP-2 as a function of $\gamma'$ for total sample size $N = 200$, (a) for a single value of $N_L = 50$ and (b) for multiple values of $N_L$.}
% \end{figure}
% 
% Notice, however, that the three parameters discussed, $\gamma'$, $N$, and $N_L$ do not play the same role in the LRLP-2.
% The first two of these parameters are also parameters of the LRLP-2 problem (\ref{eq:lrlp_two_stage}).
% The third, $N_L$, is computed from the optimal solution of (\ref{eq:lrlp_two_stage}) via (\ref{eq:kkt}) and (\ref{eq:cost_decrease_cond}).
% As such, $N_L$ should be viewed as changing with $\gamma'$.
% In general, $N_L$ will decrease as $\gamma'$ increases. To see this, recall that values of  $\gamma'$ close to $1$ consider increasingly limited set of distributions, the ones closest to the maximum likelihood distribution $p_\omega = \frac{N_\omega}{N}$, $\forall \omega$. So, the condition given in (\ref{eq:cost_decrease_cond}) is satisfied for a smaller number of scenarios. As $N_L$ changes, the estimated bound on the probability of cost decrease will have one or more jump discontinuities, moving from one line to another line below it in Figure \ref{fig:prob_cost_decrease_gp_multiple}, as seen in Figure \ref{fig:water_prob_decrease}.
% This behavior is studied in Section \ref{sec:comp_results}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Decomposition-Based Solution Method} \label{sec:soln_algorithm}

As the model gets larger, as in our water application presented in Section~\ref{sec:comp_results}, a direct solution of LRLP-2 becomes computationally expensive. 
Decomposition-based methods could significantly reduce the solution time and allow for larger problems to be solved efficiently. In this section, we propose a Bender's decomposition-based method for solving LRLP-2.
Our algorithm uses the LRLP-2 second-stage problems $h_\omega(\x,\lambda,\mu)$ (\ref{eq:lrlp_second_stage}) to form the cuts, ensuring a linear master problem.
The algorithm removes constraint (\ref{eq:lrlp_feas_constraint}) from the second-stage problem (\ref{eq:lrlp_second_stage}) and exchanges it with a series of feasibility constraints (or cuts) in the first-stage problem.
Making this change ensures that the second-stage problems are solved using the formulation of $h^\dagger_\omega(\x)$ for SLP-2 given in  (\ref{eq:slp_second_stage}), and is more efficient.
The master problem is given by
\begin{align}
	\min_{\x,\lambda,\mu} \ & \c\x + \mu + \bar{N}\lambda + \theta \label{eq:master_problem}\\
	\st \ & A\x = \b \nonumber \\
	& \theta \geq T_j (\x,\lambda,\mu)^T + t_j, \ j \in J \nonumber \\
	& \mu \geq M_k \x + m_k, \ k \in K \nonumber \\
	& \x,\lambda \geq 0, \nonumber
\end{align}
where $T_j (\x,\lambda,\mu)^T + t_j$ are the objective cuts, $M_k \x + m_k$ are the feasibility cuts on constraint (\ref{eq:lrlp_feas_constraint}), and $J$ and $K$ are the sets of objective and feasibility cuts, respectively.
We next discuss these cuts in more detail.

\subsection{Objective Cuts}

Let $(\xh,\lh,\mh)$ be the candidate solution from the master problem (\ref{eq:master_problem}).
An objective cut can be computed by solving the SLP-2 subproblems $h^\dagger_\omega(\xh)$ along with optimal dual solutions $\pi^{*,\omega}$ to each second-stage problem, and using these to compute the partial (sub)derivatives of the LRLP-2 subproblems as
\begin{align*}
	\dfrac{\partial h_\omega}{\partial \x}(\xh,\lh,\mh) & = \left(\dfrac{N\lh}{\mh - h^\dagger_\omega(\xh)}\right) \pi^{*,\omega} B^\omega \\
	\dfrac{\partial h_\omega}{\partial \lambda}(\xh,\lh,\mh) & = N + N\log\lh - N \log(\mh - h^\dagger_\omega(\xh)) \\
	\dfrac{\partial h_\omega}{\partial \mu}(\xh,\lh,\mh) & = \dfrac{-N\lh}{\mh - h^\dagger_\omega(\xh)}
\end{align*}
Recall that $h^\dagger_\omega(\x) = \min_{y^\omega \geq 0} \{\q^\omega y^\omega | D^\omega y^\omega = \d^\omega + B^\omega \x\}$.
The cuts are then given by
\begin{align*}
	T_j^\omega & = 
	\left( \begin{array}{ccc}
		\left(\frac{N\lh}{\mh - h^\dagger_\omega(\xh)}\right) \pi^{*,\omega} B^\omega, 
			 & N + N\log\lh - N\log(\mh - h^\dagger_\omega(\xh)), 
			 & -\frac{N\lh}{\mh - h^\dagger_\omega(\xh)}
	\end{array} \right) \\
	t_j^\omega & = \frac{N \lh h^\dagger_\omega(\xh) - N \lh \pi^{*,\omega}B\xh}{\mh - h^\dagger_\omega(\xh)}.
\end{align*}

For the single-cut master problem proposed, $T_j = \sum_\omega \frac{N_\omega}{N} T_j^\omega$ and $t_j = \sum_\omega \frac{N_\omega}{N} t_j^\omega$.

\subsection{Feasibility Cuts}
After the subproblems $h^\dagger_\omega(\xh)$ are solved, it may be the case that $\mh - h^\dagger_\omega(\xh) < 0$ for some $\omega$, rendering $\mh$ infeasible.
This is corrected using the feasibility problem
\begin{align*}
	U_\omega(\x,\mu) = \min_{y_\omega,z \geq 0} \ & z \\
	\st \ & z + \mu - \q^\omega y^\omega \geq 0 \\
	& D^\omega y^\omega = \d^\omega + B^\omega \x,
\end{align*}
which is solved by $z^* = h^\dagger_\omega(\x) - \mu$.
The subdifferentials can be easily found as $\frac{\partial z^*}{\partial \x} = \pi^{*,\omega} B^\omega$ and $\frac{\partial z^*}{\partial \mu} = -1$.
Then for infeasible candidate solution $(\xh,\lh,\mh)$ we get the inequality
\begin{align*}
	U_\omega(\x,\mu) \geq \pi^{*,\omega}B^\omega(\x-\xh) - (\mu -\mh) + h^\dagger_\omega(\xh) - \mh,
\end{align*}
and setting $U_\omega(\x,\mu) = 0$ to find a feasible solution gives the feasibility cut
\[
	\mu \geq \pi^{*,\omega}B^\omega \x + (h^\dagger_\omega(\xh) - \pi^{*,\omega}B^\omega\xh).
\]

Once the feasibility cut is generated, we may need to find a feasible (and reasonable) value of $\mu$ to generate an objective cut, or to initialize the next iteration of the master problem.
This can be done quickly by minimizing the objective function of (\ref{eq:lrlp_det_equiv}) with respect to $\mu$ while keeping $\xh$ and $\lh$ constant, which is equivalent to minimizing $\mu - \sum_\omega N_\omega \lh \log(\mu - h^\dagger_\omega(\xh))$.
We do this by solving the equation $\sum_\omega \frac{N_\omega \lh}{\mu - h^\dagger_\omega(\xh)} = 1$ with Newton's method.

\subsection{Decomposition Algorithm}

We solve the LRLP-2 with the following decomposition algorithm with tolerance level $\texttt{TOL}$

\begin{algorithmic}
	\State Initialize $z_l = -\infty, z_u = \infty$
	\State Solve first stage (\ref{eq:master_problem}) with $\theta = 0$  to generate $\x$
	\State Solve all second stage scenarios $h^\dagger_\omega(\x)$ (\ref{eq:slp_second_stage})
	\State Initialize $\lambda \gets 1$, $\mu$ that minimizes $\mu - \sum_\omega N_\omega \lh \log(\mu - h^\dagger_\omega(\xh))$
	\State Generate initial objective cut
	\While{$z_u - z_l \geq \texttt{TOL}\min\{|z_u|,|z_l|\}$}
		\State Solve master problem (\ref{eq:master_problem}), get $\x$,$\lambda$,$\mu$,$\theta_M$
		\State Solve sub-problems $h^\dagger_\omega(\x)$ (\ref{eq:slp_second_stage})
		\State $\theta_{\text{true}} \gets \sum_{\omega=1}^n \frac{N_\omega}{N} h_\omega(\x,\lambda,\mu)$
		\If{$\mu < \max_\omega h^\dagger_\omega(\x)$}
			\State Generate feasibility cut
			\State Find $\mu$ that minimizes $\mu - \sum_\omega N_\omega \lh \log(\mu - h^\dagger_\omega(\xh))$
		\Else
			\State $z_l \gets$ master optimal cost $\c\x + \mu + \bar{N}\lambda + \theta_{\text{true}}$
		\EndIf
		\State Generate objective cut
		\If{$\c\x + \mu + \bar{N}\lambda + \theta_{\text{true}} < z_u$}
			\State $z_u \gets \c\x + \mu + \bar{N}\lambda + \theta_{\text{true}}$
			\State $\x_\text{best} \gets \x, \lambda_\text{best} \gets \lambda, \mu_\text{best} \gets \mu$
			\State $p_\omega \gets \frac{\lambda_\text{best} N_\omega}{\mu_\text{best} - h^\dagger_\omega(\x_\text{best})}$ for $i = 1, \dots, n$
		\EndIf
	\EndWhile
\end{algorithmic}

\subsection{Computational Enhancements}

In order to enhance the performance of the above decomposition-based algorithm, we made some adjustments.
First, we included an $L_\infty$-norm trust region which is scaled up (by a factor of $3$) or down (by a factor of $\tfrac{1}{4}$) when the trust region inhibits finding the optimal solution or when the polyhedral lower approximation is far from the second-stage expected cost, respectively.
The trust region is an implementation of Algorithm 4.1 in \cite{nocedal1999numerical}.

Because we are also interested in the worst-case LRO probabilities given in the primal variables and not computed directly, we include a second tolerance as a stopping condition, ensuring that $\left| 1 - \sum_{i=1}^n p_\omega \right| < \texttt{TOL2}$ when the algorithm is completed.
This must be satisfied in addition to the original condition $z_u - z_l < \texttt{TOL}\min\{|z_u|,|z_l|\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotic Analysis of LRLP-2}

We now wish to show that the optimal value and solution of LRLP-2 converges to the optimal value and solution of the corresponding SLP-2 with the (unknown) true distribution $\ptrue$.
We begin by showing that the worst-case distribution converges weakly to the true distribution as $N \rightarrow \infty$.

Let the probability space $(\Xi,{\cal F},\P^\infty)$ be the space associated with taking infinitely many samples from the distribution $\ptrue$.
Let $\Xi' \subset \Xi$ be a measure 1 set such that $\hat{p}^N_\omega(\xi) \rightarrow \ptrue_\omega$ for all $\omega = 1, \dots, n$.

\begin{proposition} \label{prop:weak_conv}
	Let $p \neq \ptrue$.  For all $\epsilon > 0$ and $\xi \in \Xi'$ there exists $N'$ such that $\forall N \geq N'$ $D_{KL}(\hat{p}^N,p) \leq \frac{1}{N} \log\left(\frac{1}{\gamma'}\right) \Rightarrow \max_\omega |p_\omega - \ptrue_\omega| \leq \epsilon$.
\end{proposition}

\begin{proof}
	Without loss of generality, we assume that $\ptrue_\omega > 0$ for all $\omega$.
	This is valid because $\ptrue_\omega = 0 \Rightarrow \hat{p}^N_\omega = 0$, which implies $p_\omega = 0$ in the worst case distribution by (\ref{eq:kkt}).
	For simplicity, we additionally assume $\epsilon$ is chosen so that $\ptrue_\omega > \frac{\epsilon}{2}$ for all $\omega$.
	
	First, we note that $\max_\omega |p_\omega - \ptrue_\omega| \leq \max_\omega |p_\omega - \hat{p}^N_\omega| + \max_\omega |\hat{p}^N_\omega - \ptrue_\omega|$.
	Let $N''$ be such that $\max_\omega |\hat{p}^N_\omega - \ptrue_\omega| \leq \frac{\epsilon}{2}$ for all $N \geq N''$.
	
	Next, we use the alternate form of the KL divergence
	\[
		D_{KL}(\hat{p}^N,p) = \sum_{\omega=1}^n \hat{p}^N_\omega \left( - \log \frac{p_\omega}{\hat{p}^N_\omega} + \frac{p_\omega}{\hat{p}^N_\omega} - 1 \right),
	\]
	and use the fact that each term in parentheses $\phi(t) = -\log t + t - 1$ is convex, non-negative for $t \geq 0$, and $\phi(1) = 0$.
	
	To complete the proof, we will show that one can choose $N' \geq N''$ such that $\max_\omega |p_\omega - \hat{p}^N_\omega| > \frac{\epsilon}{2} \Rightarrow D_{KL}(\hat{p}^N,p) > \frac{1}{N'} \log\left(\frac{1}{\gamma'}\right)$.
	First, bound the KL divergence by
	\begin{align*}
		D_{KL}(\hat{p}^N,p) & = \sum_{\omega=1}^n \hat{p}^N_\omega \phi\left( \frac{p_\omega}{\hat{p}^N_\omega} \right) \\
		& \geq \min_\omega \{\hat{p}^N_\omega\} \cdot \max_\omega \left\{ \phi \left( \frac{p_\omega}{\hat{p}^N_\omega} \right) \right\} \\
		& \geq \min_\omega \{\hat{p}^N_\omega\} \cdot \min\left\{ \phi\left(1+\frac{\epsilon}{2}\right), \phi\left(1-\frac{\epsilon}{2}\right) \right\} \\
		& \geq \min_\omega \left\{ \ptrue_\omega - \frac{\epsilon}{2} \right\} \cdot \min\left\{ \phi\left(1+\frac{\epsilon}{2}\right), \phi\left(1-\frac{\epsilon}{2}\right) \right\},
	\end{align*}
	where the second inequality is true because $\phi \left( \frac{p_\omega}{\hat{p}^N_\omega} \right) \geq \min\left\{ \phi\left( \frac{\hat{p}^N_\omega+\tfrac{\epsilon}{2}}{\hat{p}^N_\omega} \right), \phi\left( \frac{\hat{p}^N_\omega-\tfrac{\epsilon}{2}}{\hat{p}^N_\omega} \right) \right\}$ for at least one $\omega$, and applying the inequalities $\frac{a+\eta}{a} \geq 1 + \eta$ and $\frac{a-\eta}{a} \leq 1-\eta$.
	Then choose $N'$ to satisfy $\min_\omega \left\{ \ptrue_\omega - \frac{\epsilon}{2} \right\} \cdot \min\left\{ \phi\left(1+\frac{\epsilon}{2}\right), \phi\left(1-\frac{\epsilon}{2}\right) \right\} \geq \frac{1}{N'} \log\left(\frac{1}{\gamma'}\right)$.
\end{proof}

Proposition \ref{prop:weak_conv} shows that the worst-case distributions of (\ref{eq:lrlp_primal}) converge weakly to $\ptrue$.
In the next theorem, we establish the proof that the optimal value and solution of LRLP-2 converges to that of the SLP-2 with distribution $\ptrue$ by establishing the epiconvergence of LRLP-2 to SLP-2.
\begin{theorem}
	LRLP-2 (\ref{eq:lrlp_two_stage}) epiconverges to SLP-2 (\ref{eq:slp_first_stage}) with distribution $p = \ptrue$.
\end{theorem}

\begin{proof}
	To establish the epiconvergence, we need only to apply the result of Proposition \ref{prop:weak_conv} to Theorem 3.7 of \cite{dupacova1988asymptotic}, which establishes the epiconvergence of (\ref{eq:lrlp_primal}) under the evident conditions that the objective function (under the worst-case distribution) is continuous with respect to $\omega$ and lower semicontinuous and locally lower Lipschitz with respect to $x$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application and Computational Results} \label{sec:comp_results}

Using the generalized network model (\ref{eq:gen_network_two_stage}) of Colorado River water allocation in the southeastern portion of Tucson, we created a likelihood robust water allocation problem of the form LRLP-2.
The southeastern portion of Tucson is a newly developing area and is expected to grow considerably. 
The LRLP-2 water allocation model would help authorities with future water plans in this area while being robust to uncertainties in water supplies and demands.

The model has a total of $P = 41$ time periods, representing years 2010--2050. 
%, with projections coming from studies WISP \cite{??} and TAZ \cite{??}.
% We use $P_1 = 10$ time periods for the first stage, and four second-stage scenarios are constructed from high (WISP) and low (TAZ) population estimates along with high and low estimates for the amount of water available for use.
For each time period, the network has 62 nodes representing demand for potable and nonpotable (reclaimed) water, pumps, water treatment plants, and the available water supply from the Colorado River.
The network in each time period has 102 arcs, representing the pipe network carrying the water between the nodes physically and connecting the network to the five reservoirs that connect the time stages in the model.
We use $P_1 = 10$ time periods for the first stage.
Uncertainty in the second stage takes the form of uncertain population (thus, demand for water) and supply of water.
There are a total of 4 scenarios considered in this test instance: (i) high population, high supply, (ii) high population, low supply, (iii) low population, high supply, and (iv) low population, low supply.
Each scenario is assumed to have five observations.
The high population scenarios are more costly as the system needs to meet demand or pay for unmet demand.
The low population scenarios, on the other hand, are not as costly. 
% The supply variability seems to have a nominal effect.
We applied the decomposition-based solution algorithm presented in Section \ref{sec:soln_algorithm} to solve this model and selected tolerances $\texttt{TOL} = 10^{-5}$ and $\texttt{TOL2} = 10^{-3}$ for our computational experiments.

Figure \ref{fig:worst_case} shows how the worst-case distribution changes with $\gamma'$.
When $\gamma'$ is close to $1$, we use the maximum likelihood distribution, which has equal $\tfrac{1}{4}$ probabilities on each of the four scenarios.
As $\gamma'$ is decreased, the ambiguity set increases, and the worst-case distribution used by LRLP-2 changes.
It gives higher than $\tfrac{1}{4}$ probability to the two high-population scenarios and lower than $\tfrac{1}{4}$ probability to the two low-population scenarios, making the solution more robust to costly scenarios.
Note that the scenarios fall into two similar pairs because the cost of each scenario depends strongly on the projected demand but only weakly on the projected supply of Colorado River water.
A closer look at the optimal solutions reveals that as $\gamma'$ is decreased, or as robustness is increased, the solution uses more and more reclaimed water (treated wastewater that is reused for nonpotable purposes such as irrigation) in an effort to meet demands in a least-costly way.

\begin{figure}
	\centering
	\includegraphics[width=.5\textwidth]{images/worst_case_probability}
	\caption{Worst-case distribution for the likelihood robust water allocation problem.}
	\label{fig:worst_case}
\end{figure}

%\subsection{Value of Data}

% The results of the water model were then analyzed with the value of data techniques from Section \ref{sec:value}.
% Figure \ref{fig:water_prob_decrease} shows the estimated probability that an additional sample will remove the worst-case distribution from the likelihood region, resulting in a lower-cost solution. %, analogous the results of Figure \ref{fig:prob_cost_decrease_gp}.
% The dashed line in Figure \ref{fig:water_prob_decrease} depicts the computed values of $\frac{N_L}{N}$, which provide an upper bound on the estimated probabilities. 
% Because the low-population scenarios have lower costs, an additional sample of either low-population scenario will result in a lower expected cost.
% This is what we see through most of the computed values of $\gamma'$, with $\frac{N_L}{N} = 0.5$, indicating that the sufficient condition (\ref{eq:cost_decrease_cond}) was satisfied for both low-population scenarios.
% For extremely large values of $\gamma'$---above $0.97$---we see the ratio $\frac{N_L}{N}$ quickly drops to zero.
% This occurs because (\ref{eq:cost_decrease_cond}) only compares scenario probabilities in the empirical ($N_\omega / N$) and worst-case ($p_\omega$) distributions and does not use the computed cost of the scenarios.
% As $\gamma'$ increases and the ambiguity set shrinks, the worst-case probabilities become so close the empirical probabilities that (\ref{eq:cost_decrease_cond}) can no longer be satisfied, resulting in $\frac{N_L}{N}$ decreasing to zero.
% 
% \begin{figure}
% 	\centering
% 	\includegraphics[width=.5\textwidth]{images/water_prob_decrease}
% 	\caption{Probability that an additional sample causes a decrease in worst-case expected cost for the likelihood robust water allocation problem.  The red line shows the upper bound probability $\tfrac{N_L}{N}$.}
% 	\label{fig:water_prob_decrease}
% \end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}
\label{sec:concl}

In this paper, we proposed an extension of the Likelihood Robust Optimization (LRO) method of Wang et al.\ \cite{wang2010likelihood} to general two-stage stochastic programs with recourse, creating a two-stage likelihood robust program with recourse, denoted LRLP-2.
The LRO models use the empirical likelihood function to define an ambiguity set of probability distributions using observed data and optimize the worst-case expected cost with respect to this likelihood ambiguity set.
We provided a computationally simple method to estimate the probability that an additional sample will produce a likelihood ambiguity set that does not contain the current worst-case distribution and will result in a lower-cost solution. 
We have also provided a Bender's decomposition-based solution algorithm for the LRLP-2 and applied this method to planning future water distribution in Tucson, Arizona.

Our future work includes the following. We plan to augment the existing model first with a richer set of second-stage scenarios.
In addition to more varied estimates for future population, we will integrate climate change predictions into the model to generate scenarios for future water supply from the Colorado River. 
This model is intended to include a facility location problem to determine the best places for an additional waste water treatment plants to increase the use of reclaimed water in the most cost-efficient manner. On the methodological side, we plan to provide guidelines on selecting the robustness parameter $\gamma'$ with respect to the sample size and investigate the asymptotic behavior of the model as the sample size increases. 

\section*{Acknowledgements}
Support provided by a Water Sustainability Program Fellowship through the Technology and Research Initiative Fund at the University of Arizona.
This work has also been partially supported by the National Science Foundation through grant CMMI-1151226.

An earlier version of this paper appeared in the Proceedings of the 2013 Industrial and Systems Engineering Research Conference \cite{love2013likelihood}.

\section*{References}

\bibliography{love_lro}



\section*{Author Biographies}

\noindent {\bf DAVID LOVE} is a graduate student in the Graduate Interdisciplinary Program in Applied Mathematics at the University of Arizona.
His research interests include distributionally robust stochastic programming and water resources management.
His email address is \url{dlove@math.arizona.edu} and his web page is \url{http://math.arizona.edu/~dlove}.\\

\noindent {\bf G\"{U}ZIN BAYRAKSAN} is an Associate Professor of Integrated Systems Engineering at the Ohio State University.
Her research interests include Monte Carlo sampling methods for stochastic programming and applications to water resources.
Her email address is \url{bayraksan.1@osu.edu} and her web page is \url{http://www-iwse.eng.ohio-state.edu/biosketch_GBayraksan.cfm}

\end{document}
